{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toddmarino/repos/the-resume-rag/venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st  \n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "# Load resume\n",
    "from toddbo.loader_utils import unzip, fetch_load_split\n",
    "\n",
    "unzip()\n",
    "\n",
    "index_name = st.secrets.pinecone.index\n",
    "OPENAI_API_KEY = st.secrets.openai.OPENAI_API_KEY\n",
    "\n",
    "# Load Pinecone\n",
    "@st.cache_resource\n",
    "def load_pinecone(_documents, embeddings=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)):\n",
    "    pc = Pinecone(api_key=st.secrets.pinecone.api_key)\n",
    "    index = pc.Index(st.secrets.pinecone.index)\n",
    "    docsearch = Pinecone.from_documents(_documents, embeddings, index_name=st.secrets.pinecone.index)\n",
    "    return docsearch\n",
    "\n",
    "def build_retriever(search_type=\"mmr\"):\n",
    "    documents = fetch_load_split()\n",
    "    vectordb = load_pinecone(documents)\n",
    "    if vectordb is not None:\n",
    "        retriever = vectordb.as_retriever(search_type=search_type)\n",
    "        return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "import time\n",
    "import openai\n",
    "from typing import Dict, List, Union\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def make_synchronous_openai_call(\n",
    "    *,\n",
    "    openai_api_key: str,\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    messages: List[Dict[str, Union[str, Dict[str, str]]]],\n",
    "    timeout_seconds: int,\n",
    "):\n",
    "    return openai.ChatCompletion.create(\n",
    "        api_key=openai_api_key,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "        max_tokens=st.secrets.openai.MAX_TOKENS,\n",
    "        temperature=temperature,\n",
    "        presence_penalty=0,\n",
    "        frequency_penalty=0,\n",
    "        logit_bias={},\n",
    "        stream=False,\n",
    "        request_timeout=timeout_seconds,\n",
    "    )\n",
    "\n",
    "def retrieve_resume_documents(\n",
    "        llm, \n",
    "        user_prompt, \n",
    "        retriever) -> list:\n",
    "    retriever_from_llm = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "    unique_docs = retriever_from_llm.get_relevant_documents(query=user_prompt)\n",
    "    return unique_docs\n",
    "\n",
    "def generate_search_results(\n",
    "    *,\n",
    "    retriever,\n",
    "    llm,\n",
    "    user_prompt: str,\n",
    "    timeout_seconds: int=90,\n",
    ") -> str:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    documents = retrieve_resume_documents(llm, user_prompt, retriever)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You're an assistant tasked with helping users by finding relevant documents. \"\n",
    "                \"Your task is to provide as many relevant documents as possible \"\n",
    "                \"while providing main key points on why each document is relevant as well provide its source. \"\n",
    "                \"Lastly, generating results swiftly should be prioritized over achieving perfection.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I'll provide input as text of a list of Documents in content that follows '!!!. \"\n",
    "            \"Each item in the list contains page_content and metadata.\"\n",
    "            \"provide key facts per page and give the section from the metadata.\" \n",
    "            \" Provide the information in short bullet points and provide the metadata with each document laid as such:\"\n",
    "            \"if a word is between * and *, make the word appear bold.\"\n",
    "            \"*Key Facts per Page*: \"\n",
    "            \"*Section*: \"\n",
    "            \"Do not make stuff up. If a document has no valuable information, skip it.\"\n",
    "            f\"Here is the input !!!\\n{str(documents)}\",\n",
    "        },\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    openai_response = make_synchronous_openai_call(\n",
    "        openai_api_key=st.secrets.OPENAI_API_KEY,\n",
    "        model=st.secrets.openai.OPENAI_MODEL,\n",
    "        temperature=st.secrets.openai.temperature,\n",
    "        messages=messages,\n",
    "        timeout_seconds=timeout_seconds,\n",
    "    )\n",
    "    spent_time = time.time() - start_time\n",
    "    print(f\"Search took {spent_time} seconds\")\n",
    "    return openai_response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 19:33:08.455 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/toddmarino/repos/the-resume-rag/venv/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:264\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     cached_result \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39;49mread_result(value_key)\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_cache_hit(cached_result)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_resource_api.py:498\u001b[0m, in \u001b[0;36mResourceCache.read_result\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache:\n\u001b[1;32m    497\u001b[0m     \u001b[39m# key does not exist in cache.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m CacheKeyNotFoundError()\n\u001b[1;32m    500\u001b[0m multi_results: MultiCacheResults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache[key]\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:312\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     cached_result \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39;49mread_result(value_key)\n\u001b[1;32m    313\u001b[0m     \u001b[39m# Another thread computed the value before us. Early exit!\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_resource_api.py:498\u001b[0m, in \u001b[0;36mResourceCache.read_result\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache:\n\u001b[1;32m    497\u001b[0m     \u001b[39m# key does not exist in cache.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m CacheKeyNotFoundError()\n\u001b[1;32m    500\u001b[0m multi_results: MultiCacheResults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache[key]\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mopenai\u001b[39m.\u001b[39mtemperature, model_name\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mopenai\u001b[39m.\u001b[39mgeneration_model)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Build the retriever\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m retriever \u001b[39m=\u001b[39m build_retriever()\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mbuild_retriever\u001b[0;34m(search_type)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_retriever\u001b[39m(search_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmmr\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     24\u001b[0m     documents \u001b[39m=\u001b[39m fetch_load_split()\n\u001b[0;32m---> 25\u001b[0m     vectordb \u001b[39m=\u001b[39m load_pinecone(documents)\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m vectordb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         retriever \u001b[39m=\u001b[39m vectordb\u001b[39m.\u001b[39mas_retriever(search_type\u001b[39m=\u001b[39msearch_type)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:212\u001b[0m, in \u001b[0;36mmake_cached_func_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(info\u001b[39m.\u001b[39mfunc)\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mreturn\u001b[39;00m cached_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:241\u001b[0m, in \u001b[0;36mCachedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mshow_spinner \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mshow_spinner, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[39mwith\u001b[39;00m spinner(message, cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 241\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_or_create_cached_value(args, kwargs)\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:267\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_cache_hit(cached_result)\n\u001b[1;32m    266\u001b[0m \u001b[39mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:321\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[1;32m    317\u001b[0m     \u001b[39m# We acquired the lock before any other thread. Compute the value!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mcached_message_replay_ctx\u001b[39m.\u001b[39mcalling_cached_function(\n\u001b[1;32m    319\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfunc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mallow_widgets\n\u001b[1;32m    320\u001b[0m     ):\n\u001b[0;32m--> 321\u001b[0m         computed_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# We've computed our value, and now we need to write it back to the cache\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39m# along with any \"replay messages\" that were generated during value computation.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mcached_message_replay_ctx\u001b[39m.\u001b[39m_most_recent_messages\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mload_pinecone\u001b[0;34m(_documents, embeddings)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@st\u001b[39m\u001b[39m.\u001b[39mcache_resource\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pinecone\u001b[39m(_documents, embeddings\u001b[39m=\u001b[39mOpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39mOPENAI_API_KEY)):\n\u001b[0;32m---> 17\u001b[0m     pinecone\u001b[39m.\u001b[39;49minit(api_key\u001b[39m=\u001b[39;49mst\u001b[39m.\u001b[39;49msecrets\u001b[39m.\u001b[39;49mpinecone\u001b[39m.\u001b[39;49mapi_key)\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m index_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pinecone\u001b[39m.\u001b[39mlist_indexes():\n\u001b[1;32m     19\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/pinecone/deprecation_warnings.py:38\u001b[0m, in \u001b[0;36minit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m    import os\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m        )\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=st.secrets.openai.temperature, model_name=st.secrets.openai.generation_model)\n",
    "\n",
    "# Build the retriever\n",
    "retriever = build_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhere did Todd work in 2021?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m documents \u001b[39m=\u001b[39m retrieve_resume_documents(llm, user_prompt, retriever)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Where did Todd work in 2021?\"\n",
    "documents = retrieve_resume_documents(llm, user_prompt, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_load_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:264\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     cached_result \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39;49mread_result(value_key)\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_cache_hit(cached_result)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_resource_api.py:498\u001b[0m, in \u001b[0;36mResourceCache.read_result\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache:\n\u001b[1;32m    497\u001b[0m     \u001b[39m# key does not exist in cache.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m CacheKeyNotFoundError()\n\u001b[1;32m    500\u001b[0m multi_results: MultiCacheResults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache[key]\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:312\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     cached_result \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39;49mread_result(value_key)\n\u001b[1;32m    313\u001b[0m     \u001b[39m# Another thread computed the value before us. Early exit!\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_resource_api.py:498\u001b[0m, in \u001b[0;36mResourceCache.read_result\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache:\n\u001b[1;32m    497\u001b[0m     \u001b[39m# key does not exist in cache.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m CacheKeyNotFoundError()\n\u001b[1;32m    500\u001b[0m multi_results: MultiCacheResults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache[key]\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectordb \u001b[39m=\u001b[39m load_pinecone(documents)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:212\u001b[0m, in \u001b[0;36mmake_cached_func_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(info\u001b[39m.\u001b[39mfunc)\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mreturn\u001b[39;00m cached_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:241\u001b[0m, in \u001b[0;36mCachedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mshow_spinner \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mshow_spinner, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[39mwith\u001b[39;00m spinner(message, cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 241\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_or_create_cached_value(args, kwargs)\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:267\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_cache_hit(cached_result)\n\u001b[1;32m    266\u001b[0m \u001b[39mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:321\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[1;32m    317\u001b[0m     \u001b[39m# We acquired the lock before any other thread. Compute the value!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mcached_message_replay_ctx\u001b[39m.\u001b[39mcalling_cached_function(\n\u001b[1;32m    319\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfunc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mallow_widgets\n\u001b[1;32m    320\u001b[0m     ):\n\u001b[0;32m--> 321\u001b[0m         computed_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# We've computed our value, and now we need to write it back to the cache\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39m# along with any \"replay messages\" that were generated during value computation.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mcached_message_replay_ctx\u001b[39m.\u001b[39m_most_recent_messages\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mload_pinecone\u001b[0;34m(_documents, embeddings)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@st\u001b[39m\u001b[39m.\u001b[39mcache_resource\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pinecone\u001b[39m(_documents, embeddings\u001b[39m=\u001b[39mOpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39mOPENAI_API_KEY)):\n\u001b[0;32m---> 17\u001b[0m     pinecone\u001b[39m.\u001b[39;49minit(api_key\u001b[39m=\u001b[39;49mst\u001b[39m.\u001b[39;49msecrets\u001b[39m.\u001b[39;49mpinecone\u001b[39m.\u001b[39;49mapi_key)\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m index_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pinecone\u001b[39m.\u001b[39mlist_indexes():\n\u001b[1;32m     19\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/pinecone/deprecation_warnings.py:38\u001b[0m, in \u001b[0;36minit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m    import os\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m        )\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "vectordb = load_pinecone(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=st.secrets.pinecone.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(st.secrets.pinecone.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.00032,\n",
       " 'namespaces': {'v1': {'vector_count': 32}},\n",
       " 'total_vector_count': 32}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
