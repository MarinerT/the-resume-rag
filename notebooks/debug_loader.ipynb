{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toddmarino/repos/the-resume-rag/venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st  \n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "# Load resume\n",
    "from toddbo.loader_utils import unzip, fetch_load_split\n",
    "\n",
    "unzip()\n",
    "\n",
    "index_name = st.secrets.pinecone.index\n",
    "OPENAI_API_KEY = st.secrets.openai.OPENAI_API_KEY\n",
    "\n",
    "# Load Pinecone\n",
    "@st.cache_resource\n",
    "def load_pinecone(_documents, embeddings=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)):\n",
    "    pc = Pinecone(api_key=st.secrets.pinecone.api_key)\n",
    "    index = pc.Index(st.secrets.pinecone.index)\n",
    "    docsearch = Pinecone.from_documents(_documents, embeddings, index_name=st.secrets.pinecone.index)\n",
    "    return docsearch\n",
    "\n",
    "def build_retriever(search_type=\"mmr\"):\n",
    "    documents = fetch_load_split()\n",
    "    vectordb = load_pinecone(documents)\n",
    "    if vectordb is not None:\n",
    "        retriever = vectordb.as_retriever(search_type=search_type)\n",
    "        return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "import time\n",
    "import openai\n",
    "from typing import Dict, List, Union\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def make_synchronous_openai_call(\n",
    "    *,\n",
    "    openai_api_key: str,\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    messages: List[Dict[str, Union[str, Dict[str, str]]]],\n",
    "    timeout_seconds: int,\n",
    "):\n",
    "    return openai.ChatCompletion.create(\n",
    "        api_key=openai_api_key,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "        max_tokens=st.secrets.openai.MAX_TOKENS,\n",
    "        temperature=temperature,\n",
    "        presence_penalty=0,\n",
    "        frequency_penalty=0,\n",
    "        logit_bias={},\n",
    "        stream=False,\n",
    "        request_timeout=timeout_seconds,\n",
    "    )\n",
    "\n",
    "def retrieve_resume_documents(\n",
    "        llm, \n",
    "        user_prompt, \n",
    "        retriever) -> list:\n",
    "    retriever_from_llm = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "    unique_docs = retriever_from_llm.get_relevant_documents(query=user_prompt)\n",
    "    return unique_docs\n",
    "\n",
    "\n",
    "def retrieve_chroma_documents(\n",
    "    client,\n",
    "    prompt: str,\n",
    "):\n",
    "    retriever = generate_context(\n",
    "        client,\n",
    "        embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    "        collection_name=st.secrets.chroma.COLLECTION,\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=st.secrets.openai.temperature)\n",
    "    retriever_from_llm = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "    unique_docs = retriever_from_llm.get_relevant_documents(query=prompt)\n",
    "    return unique_docs\n",
    "\n",
    "def generate_search_results(\n",
    "    *,\n",
    "    llm,\n",
    "    user_prompt: str,\n",
    "    timeout_seconds: int=90,\n",
    ") -> str:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    documents = build_chroma_retriever(llm, user_prompt)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You're an personal assistant tasked with helping recruiters find relevant experience from Todd's resume. \"\n",
    "                \"Your task is to provide as many relevant documents as possible. \"\n",
    "                \"Lastly, generating results swiftly should be prioritized over achieving perfection.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I'll provide input as text of a list of Documents in content that follows '!!!. \"\n",
    "            \"Each item in the list contains page_content and metadata.\"\n",
    "            \"Provide a brief summary of all the documents.\"\n",
    "            \"Give the section from the metadata and the related content.\" \n",
    "            \"Provide the information in short bullet points and provide the metadata with each document laid as such:\"\n",
    "            \"if a word is between * and *, make the word appear bold.\"\n",
    "            \"*Summary*: \"\n",
    "            \"*Section*: \"\n",
    "            \"*Supporting Details*:\"\n",
    "            \"Do not make stuff up. If a document has no valuable information, skip it.\"\n",
    "            f\"Here is the input !!!\\n{str(documents)}\",\n",
    "        },\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    \n",
    "    openai_response = make_synchronous_openai_call(\n",
    "        openai_api_key=st.secrets.openai.OPENAI_API_KEY,\n",
    "        model=st.secrets.openai.OPENAI_MODEL,\n",
    "        temperature=st.secrets.openai.temperature,\n",
    "        messages=messages,\n",
    "        timeout_seconds=timeout_seconds,\n",
    "    )\n",
    "    spent_time = time.time() - start_time\n",
    "    print(f\"Search took {spent_time} seconds\")\n",
    "    return openai_response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "st.secrets has no attribute \"OPENAI_API_KEY\". Did you forget to add it to secrets.toml or the app settings on Streamlit Cloud? More info: https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/secrets.py:287\u001b[0m, in \u001b[0;36mSecrets.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse(\u001b[39mTrue\u001b[39;49;00m)[key]\n\u001b[1;32m    288\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Mapping):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'OPENAI_API_KEY'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# PREDEFINED\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtoddbo\u001b[39;00m \u001b[39mimport\u001b[39;00m connect_to_chroma, connect_to_collection\n\u001b[1;32m      3\u001b[0m client \u001b[39m=\u001b[39m connect_to_chroma(chroma_host\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mchroma\u001b[39m.\u001b[39mCHROMA_HOST, chroma_port\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mchroma\u001b[39m.\u001b[39mCHROMA_PORT)\n\u001b[1;32m      4\u001b[0m collection \u001b[39m=\u001b[39m connect_to_collection(client, st\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mchroma\u001b[39m.\u001b[39mCOLLECTION)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/toddbo/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mloader_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mretriever\u001b[39;00m \u001b[39mimport\u001b[39;00m build_retriever\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mchain\u001b[39;00m \u001b[39mimport\u001b[39;00m generate_search_results\n",
      "File \u001b[0;32m~/repos/the-resume-rag/toddbo/loader_utils.py:10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mst\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m OPENAI_KEY \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39;49msecrets\u001b[39m.\u001b[39;49mOPENAI_API_KEY\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munzip\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39munzips resume.zip and dumps into current directory as resume/\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/secrets.py:296\u001b[0m, in \u001b[0;36mSecrets.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m# We add FileNotFoundError since __getattr__ is expected to only raise\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m# AttributeError. Without handling FileNotFoundError, unittests.mocks\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# fails during mock creation on Python3.9\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyError\u001b[39;00m, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(_missing_attr_error_message(key))\n",
      "\u001b[0;31mAttributeError\u001b[0m: st.secrets has no attribute \"OPENAI_API_KEY\". Did you forget to add it to secrets.toml or the app settings on Streamlit Cloud? More info: https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management"
     ]
    }
   ],
   "source": [
    "# PREDEFINED\n",
    "from toddbo import connect_to_chroma, connect_to_collection\n",
    "client = connect_to_chroma(chroma_host=st.secrets.chroma.CHROMA_HOST, chroma_port=st.secrets.chroma.CHROMA_PORT)\n",
    "collection = connect_to_collection(client, st.secrets.chroma.COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_chroma_retriever() missing 1 required positional argument: 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mopenai\u001b[39m.\u001b[39mtemperature, model_name\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mopenai\u001b[39m.\u001b[39mgeneration_model)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Build the retriever\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m retriever \u001b[39m=\u001b[39m build_chroma_retriever()\n",
      "\u001b[0;31mTypeError\u001b[0m: build_chroma_retriever() missing 1 required positional argument: 'prompt'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=st.secrets.openai.temperature, model_name=st.secrets.openai.generation_model)\n",
    "\n",
    "# Build the retriever\n",
    "retriever = build_chroma_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connect_to_chroma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhere did Todd work in 2021?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m documents \u001b[39m=\u001b[39m build_chroma_retriever(user_prompt)\n",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m, in \u001b[0;36mbuild_chroma_retriever\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_chroma_retriever\u001b[39m(\n\u001b[1;32m     46\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     47\u001b[0m ):\n\u001b[0;32m---> 48\u001b[0m     client \u001b[39m=\u001b[39m connect_to_chroma(\n\u001b[1;32m     49\u001b[0m         chroma_host\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mchroma\u001b[39m.\u001b[39mCHROMA_HOST,\n\u001b[1;32m     50\u001b[0m         chroma_port\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mchroma\u001b[39m.\u001b[39mCHROMA_PORT,\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     retriever \u001b[39m=\u001b[39m generate_context(\n\u001b[1;32m     53\u001b[0m         client,\n\u001b[1;32m     54\u001b[0m         embedding_function\u001b[39m=\u001b[39mOpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39mOPENAI_API_KEY),\n\u001b[1;32m     55\u001b[0m         collection_name\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mchroma\u001b[39m.\u001b[39mCOLLECTION,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39mst\u001b[39m.\u001b[39msecrets\u001b[39m.\u001b[39mopenai\u001b[39m.\u001b[39mtemperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'connect_to_chroma' is not defined"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Where did Todd work in 2021?\"\n",
    "documents = build_chroma_retriever(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_load_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:264\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     cached_result \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39;49mread_result(value_key)\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_cache_hit(cached_result)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_resource_api.py:498\u001b[0m, in \u001b[0;36mResourceCache.read_result\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache:\n\u001b[1;32m    497\u001b[0m     \u001b[39m# key does not exist in cache.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m CacheKeyNotFoundError()\n\u001b[1;32m    500\u001b[0m multi_results: MultiCacheResults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache[key]\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:312\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     cached_result \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39;49mread_result(value_key)\n\u001b[1;32m    313\u001b[0m     \u001b[39m# Another thread computed the value before us. Early exit!\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_resource_api.py:498\u001b[0m, in \u001b[0;36mResourceCache.read_result\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache:\n\u001b[1;32m    497\u001b[0m     \u001b[39m# key does not exist in cache.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m CacheKeyNotFoundError()\n\u001b[1;32m    500\u001b[0m multi_results: MultiCacheResults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mem_cache[key]\n",
      "\u001b[0;31mCacheKeyNotFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectordb \u001b[39m=\u001b[39m load_pinecone(documents)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:212\u001b[0m, in \u001b[0;36mmake_cached_func_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(info\u001b[39m.\u001b[39mfunc)\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mreturn\u001b[39;00m cached_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:241\u001b[0m, in \u001b[0;36mCachedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mshow_spinner \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mshow_spinner, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[39mwith\u001b[39;00m spinner(message, cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 241\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_or_create_cached_value(args, kwargs)\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:267\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_cache_hit(cached_result)\n\u001b[1;32m    266\u001b[0m \u001b[39mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:321\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[1;32m    317\u001b[0m     \u001b[39m# We acquired the lock before any other thread. Compute the value!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mcached_message_replay_ctx\u001b[39m.\u001b[39mcalling_cached_function(\n\u001b[1;32m    319\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfunc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mallow_widgets\n\u001b[1;32m    320\u001b[0m     ):\n\u001b[0;32m--> 321\u001b[0m         computed_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# We've computed our value, and now we need to write it back to the cache\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39m# along with any \"replay messages\" that were generated during value computation.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mcached_message_replay_ctx\u001b[39m.\u001b[39m_most_recent_messages\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mload_pinecone\u001b[0;34m(_documents, embeddings)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@st\u001b[39m\u001b[39m.\u001b[39mcache_resource\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pinecone\u001b[39m(_documents, embeddings\u001b[39m=\u001b[39mOpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39mOPENAI_API_KEY)):\n\u001b[0;32m---> 17\u001b[0m     pinecone\u001b[39m.\u001b[39;49minit(api_key\u001b[39m=\u001b[39;49mst\u001b[39m.\u001b[39;49msecrets\u001b[39m.\u001b[39;49mpinecone\u001b[39m.\u001b[39;49mapi_key)\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m index_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pinecone\u001b[39m.\u001b[39mlist_indexes():\n\u001b[1;32m     19\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/the-resume-rag/venv/lib/python3.11/site-packages/pinecone/deprecation_warnings.py:38\u001b[0m, in \u001b[0;36minit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m    import os\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m        )\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "vectordb = load_pinecone(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=st.secrets.pinecone.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(st.secrets.pinecone.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.00032,\n",
       " 'namespaces': {'v1': {'vector_count': 32}},\n",
       " 'total_vector_count': 32}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
